{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we will work with text data from newsgroup postings on a variety of topics. We will train classifiers to distinguish between the topics based on the text of the posts. Whereas with digit classification, the input is relatively dense: a 28x28 matrix of pixels, many of which are non-zero, here we'll represent each document with a \"bag-of-words\" model. As you'll see, this makes the feature representation quite sparse -- only a few words of the total vocabulary are active in any given document. The bag-of-words assumption here is that the label depends only on the words; their order is not important.\n",
    "\n",
    "The SK-learn documentation on feature extraction:\n",
    "http://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn import preprocessing\n",
    "#from sklearn.feature_extraction import SelectPercentile\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn library for importing the newsgroup data.\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data, stripping out metadata so that we learn classifiers that only use textual features. By default, newsgroups data is split into train and test sets. We further split the test so we have a dev set. Note that we specify 4 categories to use for this project. If you remove the categories argument from the fetch function, you'll get all 20 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training label shape: (2034,)\n",
      "test label shape: (676,)\n",
      "dev label shape: (676,)\n",
      "labels names: ['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',\n",
    "                                      remove=('headers', 'footers', 'quotes'),\n",
    "                                      categories=categories)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test',\n",
    "                                     remove=('headers', 'footers', 'quotes'),\n",
    "                                     categories=categories)\n",
    "\n",
    "num_test = len(newsgroups_test.target)\n",
    "test_data, test_labels = newsgroups_test.data[677:], newsgroups_test.target[677:]\n",
    "dev_data, dev_labels = newsgroups_test.data[:676], newsgroups_test.target[:676]\n",
    "train_data, train_labels = newsgroups_train.data, newsgroups_train.target\n",
    "\n",
    "print ('training label shape:', train_labels.shape)\n",
    "print ('test label shape:', test_labels.shape)\n",
    "print ('dev label shape:', dev_labels.shape)\n",
    "print ('labels names:', newsgroups_train.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) For each of the first 5 training examples, print the text of the message along with the label.\n",
    "\n",
    "[2 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label =  1\n",
      "[]\n",
      "Label =  3\n",
      "[\"Hi,\\n\\nI've noticed that if you only save a model (with all your mapping planes\\npositioned carefully) to a .3DS file that when you reload it after restarting\\n3DS, they are given a default position and orientation.  But if you save\\nto a .PRJ file their positions/orientation are preserved.  Does anyone\\nknow why this information is not stored in the .3DS file?  Nothing is\\nexplicitly said in the manual about saving texture rules in the .PRJ file. \\nI'd like to be able to read the texture rule information, does anyone have \\nthe format for the .PRJ file?\\n\\nIs the .CEL file format available from somewhere?\\n\\nRych\"]\n",
      "Label =  2\n",
      "[\"Hi,\\n\\nI've noticed that if you only save a model (with all your mapping planes\\npositioned carefully) to a .3DS file that when you reload it after restarting\\n3DS, they are given a default position and orientation.  But if you save\\nto a .PRJ file their positions/orientation are preserved.  Does anyone\\nknow why this information is not stored in the .3DS file?  Nothing is\\nexplicitly said in the manual about saving texture rules in the .PRJ file. \\nI'd like to be able to read the texture rule information, does anyone have \\nthe format for the .PRJ file?\\n\\nIs the .CEL file format available from somewhere?\\n\\nRych\", '\\n\\nSeems to be, barring evidence to the contrary, that Koresh was simply\\nanother deranged fanatic who thought it neccessary to take a whole bunch of\\nfolks with him, children and all, to satisfy his delusional mania. Jim\\nJones, circa 1993.\\n\\n\\nNope - fruitcakes like Koresh have been demonstrating such evil corruption\\nfor centuries.']\n",
      "Label =  0\n",
      "[\"Hi,\\n\\nI've noticed that if you only save a model (with all your mapping planes\\npositioned carefully) to a .3DS file that when you reload it after restarting\\n3DS, they are given a default position and orientation.  But if you save\\nto a .PRJ file their positions/orientation are preserved.  Does anyone\\nknow why this information is not stored in the .3DS file?  Nothing is\\nexplicitly said in the manual about saving texture rules in the .PRJ file. \\nI'd like to be able to read the texture rule information, does anyone have \\nthe format for the .PRJ file?\\n\\nIs the .CEL file format available from somewhere?\\n\\nRych\", '\\n\\nSeems to be, barring evidence to the contrary, that Koresh was simply\\nanother deranged fanatic who thought it neccessary to take a whole bunch of\\nfolks with him, children and all, to satisfy his delusional mania. Jim\\nJones, circa 1993.\\n\\n\\nNope - fruitcakes like Koresh have been demonstrating such evil corruption\\nfor centuries.', \"\\n >In article <1993Apr19.020359.26996@sq.sq.com>, msb@sq.sq.com (Mark Brader) \\n\\nMB>                                                             So the\\nMB> 1970 figure seems unlikely to actually be anything but a perijove.\\n\\nJG>Sorry, _perijoves_...I'm not used to talking this language.\\n\\nCouldn't we just say periapsis or apoapsis?\\n\\n \"]\n",
      "Label =  2\n",
      "[\"Hi,\\n\\nI've noticed that if you only save a model (with all your mapping planes\\npositioned carefully) to a .3DS file that when you reload it after restarting\\n3DS, they are given a default position and orientation.  But if you save\\nto a .PRJ file their positions/orientation are preserved.  Does anyone\\nknow why this information is not stored in the .3DS file?  Nothing is\\nexplicitly said in the manual about saving texture rules in the .PRJ file. \\nI'd like to be able to read the texture rule information, does anyone have \\nthe format for the .PRJ file?\\n\\nIs the .CEL file format available from somewhere?\\n\\nRych\", '\\n\\nSeems to be, barring evidence to the contrary, that Koresh was simply\\nanother deranged fanatic who thought it neccessary to take a whole bunch of\\nfolks with him, children and all, to satisfy his delusional mania. Jim\\nJones, circa 1993.\\n\\n\\nNope - fruitcakes like Koresh have been demonstrating such evil corruption\\nfor centuries.', \"\\n >In article <1993Apr19.020359.26996@sq.sq.com>, msb@sq.sq.com (Mark Brader) \\n\\nMB>                                                             So the\\nMB> 1970 figure seems unlikely to actually be anything but a perijove.\\n\\nJG>Sorry, _perijoves_...I'm not used to talking this language.\\n\\nCouldn't we just say periapsis or apoapsis?\\n\\n \", 'I have a request for those who would like to see Charley Wingate\\nrespond to the \"Charley Challenges\" (and judging from my e-mail, there\\nappear to be quite a few of you.)  \\n\\nIt is clear that Mr. Wingate intends to continue to post tangential or\\nunrelated articles while ingoring the Challenges themselves.  Between\\nthe last two re-postings of the Challenges, I noted perhaps a dozen or\\nmore posts by Mr. Wingate, none of which answered a single Challenge.  \\n\\nIt seems unmistakable to me that Mr. Wingate hopes that the questions\\nwill just go away, and he is doing his level best to change the\\nsubject.  Given that this seems a rather common net.theist tactic, I\\nwould like to suggest that we impress upon him our desire for answers,\\nin the following manner:\\n\\n1. Ignore any future articles by Mr. Wingate that do not address the\\nChallenges, until he answers them or explictly announces that he\\nrefuses to do so.\\n\\n--or--\\n\\n2. If you must respond to one of his articles, include within it\\nsomething similar to the following:\\n\\n    \"Please answer the questions posed to you in the Charley Challenges.\"\\n\\nReally, I\\'m not looking to humiliate anyone here, I just want some\\nhonest answers.  You wouldn\\'t think that honesty would be too much to\\nask from a devout Christian, would you?  \\n\\nNevermind, that was a rhetorical question.']\n"
     ]
    }
   ],
   "source": [
    "# P1():\n",
    "\n",
    "# Considering only five example data\n",
    "for text in range(len(train_data[:5])):\n",
    "    \n",
    "    #print label\n",
    "    print('Label = ', train_labels[text])\n",
    "    #print example text\n",
    "    print(train_data[:text])\n",
    "\n",
    "#P1(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Use CountVectorizer to turn the raw training text into feature vectors. You should use the fit_transform function, which makes 2 passes through the data: first it computes the vocabulary (\"fit\"), second it converts the raw text into feature vectors using the vocabulary (\"transform\").\n",
    "\n",
    "The vectorizer has a lot of options. To get familiar with some of them, write code to answer these questions:\n",
    "\n",
    "a. The output of the transform (also of fit_transform) is a sparse matrix: http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.csr_matrix.html. What is the size of the vocabulary? What is the average number of non-zero features per example? What fraction of the entries in the matrix are non-zero? Hint: use \"nnz\" and \"shape\" attributes.\n",
    "\n",
    "b. What are the 0th and last feature strings (in alphabetical order)? Hint: use the vectorizer's get_feature_names function.\n",
    "\n",
    "c. Specify your own vocabulary with 4 words: [\"atheism\", \"graphics\", \"space\", \"religion\"]. Confirm the training vectors are appropriately shaped. Now what's the average number of non-zero features per example?\n",
    "\n",
    "d. Instead of extracting unigram word features, use \"analyzer\" and \"ngram_range\" to extract bigram and trigram character features. What size vocabulary does this yield?\n",
    "\n",
    "e. Use the \"min_df\" argument to prune words that appear in fewer than 10 documents. What size vocabulary does this yield?\n",
    "\n",
    "f. Using the standard CountVectorizer, what fraction of the words in the dev data are missing from the vocabulary? Hint: build a vocabulary for both train and dev and look at the size of the difference.\n",
    "\n",
    "[6 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#P2:\n",
    "\n",
    "##Convert the raw training text into feature vectors##\n",
    "cv = CountVectorizer()\n",
    "train_cv = cv.fit_transform(train_data)\n",
    "#print(train_cv)\n",
    "train_cv.toarray()\n",
    "#dev_cv = cv.transform(dev_data)\n",
    "#print(train_cv.toarray())\n",
    "\n",
    "#print(train_cv.shape, dev_cv.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the vocabulary is: 26879\n",
      "Size of the average non-zero per example is: 96.706\n",
      "Fraction of non-zero entry is: 0.0036\n"
     ]
    }
   ],
   "source": [
    "#P2.a\n",
    "\n",
    "##Size of vocabulary##\n",
    "\n",
    "print(\"Size of the vocabulary is:\", train_cv.shape[1])\n",
    "\n",
    "\n",
    "#Get total count and average count of non-zero features\n",
    "\n",
    "def count_nonzero(feature_vector):    \n",
    "    count_nnz = 0\n",
    "    avg_nnz = 0\n",
    "    for example in range(feature_vector.shape[0]):\n",
    "        count_nnz = count_nnz + feature_vector[example].nnz\n",
    "    avg_nnz = round((count_nnz/feature_vector.shape[0]), 5)\n",
    "    return[count_nnz, avg_nnz]\n",
    "\n",
    "#Divide total non-zero entry by total matrix entry\n",
    "def frac_nonzero(feature_vector, total_nonzero):\n",
    "    total_entry = feature_vector.shape[0]*feature_vector.shape[1]\n",
    "    frac_nonzero = round(total_nonzero/total_entry,5)\n",
    "    return frac_nonzero\n",
    "\n",
    "##Average Number of non-zero features##\n",
    "\n",
    "count_nnz = count_nonzero(train_cv)\n",
    "print(\"Size of the average non-zero per example is:\", count_nnz[1])\n",
    "\n",
    "##Fraction of non-zero entry in the feature matrix##\n",
    "\n",
    "frac_nnz = frac_nonzero(train_cv, count_nnz[0])\n",
    "print(\"Fraction of non-zero entry is:\", frac_nnz)\n",
    "\n",
    "#print(\"Total non-zero entry is:\", count_nonzero(train_cv)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-th feature of the string is:  00\n",
      "Last feature of the string is:  zyxel\n"
     ]
    }
   ],
   "source": [
    "###P2.b\n",
    "\n",
    "feature_names = cv.get_feature_names()\n",
    "\n",
    "##0-th feature of the string\n",
    "print(\"0-th feature of the string is: \",feature_names[0])\n",
    "\n",
    "##last feature of the string\n",
    "print(\"Last feature of the string is: \",feature_names[len(feature_names)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the average non-zero per example is: 0.26844\n"
     ]
    }
   ],
   "source": [
    "###P2.c\n",
    "\n",
    "##Specify vocabulary with 4 words: [\"atheism\", \"graphics\", \"space\", \"religion\"]\n",
    "\n",
    "my_vocab = [\"atheism\", \"graphics\", \"space\", \"religion\"]\n",
    "\n",
    "##Confirm the training vectors are appropriately shaped.\n",
    "my_cv = CountVectorizer(vocabulary=my_vocab)\n",
    "my_train_cv = my_cv.fit_transform(train_data)\n",
    "\n",
    "##The average number of non-zero features per example\n",
    "count_nnz = count_nonzero(my_train_cv)\n",
    "print(\"Size of the average non-zero per example is:\", count_nnz[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the vocabulary is:  346592\n"
     ]
    }
   ],
   "source": [
    "###P2.d\n",
    "\n",
    "##Instead of extracting unigram word features, use \"analyzer\" and \"ngram_range\" to extract bigram and trigram character features.\n",
    "\n",
    "#For extracting bigram and trigram character word features using ngram_rang = (2,3)\n",
    "tv = TfidfVectorizer(analyzer = 'word', min_df= 1, stop_words= 'english', ngram_range=(2,3))\n",
    "train_tv = tv.fit_transform(train_data)\n",
    "#feature_tv = tv.get_feature_names()\n",
    "#print(feature_tv[:20])\n",
    "#cv1 = CountVectorizer()\n",
    "#train_cv1 = cv1.fit_transform(train_data[:20])\n",
    "#print(train_cv1)\n",
    "#print('TfidfVectorizer = ',train_tv)\n",
    "#print(train_cv1.toarray())\n",
    "#print('hi')\n",
    "#print(train_tv.toarray())\n",
    "#feature_cv = cv1.get_feature_names()\n",
    "\n",
    "##What size vocabulary does this yield?\n",
    "\n",
    "#Define function to calculate vocabulary length\n",
    "\n",
    "def vocab_length(feature_vector):\n",
    "    return(len(feature_vector.vocabulary_))\n",
    "\n",
    "print('The size of the vocabulary is: ', vocab_length(tv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the vocabulary where words appear in fewer than 10 documents is:  3064\n"
     ]
    }
   ],
   "source": [
    "###P2.e\n",
    "\n",
    "##Use the \"min_df\" argument to prune words that appear in fewer than 10 documents.\n",
    "\n",
    "tv_min_df = TfidfVectorizer(analyzer = 'word', min_df= 10)\n",
    "train_tv_min_df = tv_min_df.fit_transform(train_data)\n",
    "\n",
    "##What size vocabulary does this yield?\n",
    "\n",
    "print('The size of the vocabulary where words appear in fewer than 10 documents is: ', vocab_length(tv_min_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_train_len =  26879\n",
      "vocab_dev_len =  16246\n",
      "vocab_len_diff =  10633\n",
      "fraction of the words in the dev data that are missing from the vocabulary is:end=  0.39559\n"
     ]
    }
   ],
   "source": [
    "###P2.f\n",
    "\n",
    "##f. Using the standard CountVectorizer, what fraction of the words in the dev data are missing from the vocabulary?\n",
    "#Hint: build a vocabulary for both train and dev and look at the size of the difference.\n",
    "\n",
    "#\n",
    "# Calculate length of vocabulary in train_data\n",
    "\n",
    "vocab_train_len = vocab_length(cv)\n",
    "print('vocab_train_len = ',vocab_train_len)\n",
    "# Calculate length of vocabulary in dev_data\n",
    "\n",
    "cv_dev = CountVectorizer()\n",
    "dev_cv = cv_dev.fit_transform(dev_data)\n",
    "vocab_dev_len = vocab_length(cv_dev)\n",
    "print('vocab_dev_len = ',vocab_dev_len)\n",
    "# Calculate Difference in vocabulary length between train and dev\n",
    "\n",
    "vocab_len_diff = vocab_train_len - vocab_dev_len\n",
    "print('vocab_len_diff = ',vocab_len_diff)\n",
    "\n",
    "# Calculate the fraction of the words in the dev data that are missing from the vocabulary\n",
    "\n",
    "print('fraction of the words in the dev data that are missing from the vocabulary is:end= ', round(vocab_len_diff/vocab_train_len, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) Use the default CountVectorizer options and report the f1 score (use metrics.f1_score) for a k nearest neighbors classifier; find the optimal value for k. Also fit a Multinomial Naive Bayes model and find the optimal value for alpha. Finally, fit a logistic regression model and find the optimal value for the regularization strength C using l2 regularization. A few questions:\n",
    "\n",
    "a. Why doesn't nearest neighbors work well for this problem?\n",
    "\n",
    "b. Any ideas why logistic regression doesn't work as well as Naive Bayes?\n",
    "\n",
    "c. Logistic regression estimates a weight vector for each class, which you can access with the coef\\_ attribute. Output the sum of the squared weight values for each class for each setting of the C parameter. Briefly explain the relationship between the sum and the value of C.\n",
    "\n",
    "[4 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal value for K is:  9\n",
      "F1 score for KNN classifier when k = 9 is:  0.42330050510183215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/debalinamaiti/Documents/GraduateStudy/MIDS/Python_Bridge/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/model_selection/_split.py:1943: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/Users/debalinamaiti/Documents/GraduateStudy/MIDS/Python_Bridge/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/naive_bayes.py:480: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/Users/debalinamaiti/Documents/GraduateStudy/MIDS/Python_Bridge/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/naive_bayes.py:480: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/Users/debalinamaiti/Documents/GraduateStudy/MIDS/Python_Bridge/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/naive_bayes.py:480: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best alpha value =  0.01\n",
      "best score using Multinomial NB Classifier is:  0.828416912487709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/debalinamaiti/Documents/GraduateStudy/MIDS/Python_Bridge/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/debalinamaiti/Documents/GraduateStudy/MIDS/Python_Bridge/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Users/debalinamaiti/Documents/GraduateStudy/MIDS/Python_Bridge/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/model_selection/_split.py:1943: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/Users/debalinamaiti/Documents/GraduateStudy/MIDS/Python_Bridge/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/debalinamaiti/Documents/GraduateStudy/MIDS/Python_Bridge/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best C value =  0.1\n",
      "best score using Logistic Regression is:  0.7664700098328416\n"
     ]
    }
   ],
   "source": [
    "### P3():\n",
    "## Fit a K-nearest neighbor classifier ##\n",
    "\n",
    "## Define prediction function for knn classifier ##\n",
    "def prediction(k, train, labels, dev):\n",
    "    neigh = KNeighborsClassifier(k)\n",
    "    neigh.fit(train, labels)\n",
    "    predict = neigh.predict(dev)\n",
    "    return predict\n",
    "\n",
    "## Define Function to calculate accuracy ##\n",
    "def accuracy(predict, dev_labels):\n",
    "    accurate, total = 0, 0\n",
    "    for pred, label in zip(predict, dev_labels):\n",
    "        if pred == label: accurate += 1\n",
    "        total += 1\n",
    "    return [total, accurate, round(1.0*accurate/total, 4)]\n",
    "\n",
    "k_val = [1, 3, 5, 7, 9]\n",
    "accu = 0\n",
    "bestK = 0\n",
    "\n",
    "# Using transform for dev_data instead of fit_transform as already it is fitted to train_data #\n",
    "dev_data_cv = cv.transform(dev_data)\n",
    "\n",
    "for k in k_val:\n",
    "    predict = prediction(k, train_cv, train_labels, dev_data_cv)\n",
    "    result = accuracy(predict, dev_labels)\n",
    "    if result[2] > accu:\n",
    "        #accu = result[2.0]\n",
    "        bestK = k\n",
    "print('Optimal value for K is: ', bestK)\n",
    "    \n",
    "pred = prediction(bestK, train_cv, train_labels, dev_data_cv)\n",
    "\n",
    "actual = np.array(dev_labels)\n",
    "#print(actual)\n",
    "#print(metrics.classification_report(dev_labels, pred, target_names=newsgroups_train.target_names))\n",
    "print(\"F1 score for KNN classifier when k = 9 is: \",f1_score(dev_labels, pred, average = 'macro'))\n",
    "\n",
    "## Naive Bayes ##\n",
    "\n",
    "mnb = MultinomialNB().fit(train_cv, train_labels)\n",
    "\n",
    "# prepare a range of alpha values to test\n",
    "alphas = np.array([0.0, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 10.0])\n",
    "\n",
    "#Search for best alpha value\n",
    "\n",
    "grid_mnb = GridSearchCV(estimator=mnb, param_grid=dict(alpha=alphas))\n",
    "grid_mnb.fit(train_cv, train_labels)\n",
    "\n",
    "# summarize the results of the grid search\n",
    "print('best alpha value = ', grid_mnb.best_estimator_.alpha)\n",
    "print('best score using Multinomial NB Classifier is: ', grid_mnb.best_score_)\n",
    "\n",
    "## Logistic Regression ##\n",
    "\n",
    "logit = LogisticRegression().fit(train_cv, train_labels)\n",
    "\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000] }\n",
    "grid_logit = GridSearchCV(LogisticRegression(penalty='l2'), param_grid)\n",
    "grid_logit.fit(train_cv, train_labels)\n",
    "\n",
    "print('best C value = ', grid_logit.best_estimator_.C)\n",
    "print('best score using Logistic Regression is: ', grid_logit.best_score_)\n",
    "\n",
    "#P3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:**  \n",
    "a) By extensive experimenting, through KNN clustering, consistency/ accuracy can be improved for predicting news group data. But due to overfitting, accuracy on predicting test data will not be that great.  \n",
    "  \n",
    "b) Naive Bayes is a generative model and so first models the joint distribution of the feature X and target Y; and then predicts the posterior probability given as P(y|x). Also it considers all the features to be conditionally independent.  \n",
    "On the other hand, Logistic Regression is a discriminative model. So it directly models the posterior probabilty of P(y|x) by learning the input to output mapping. Also it splits feature space linearly and predict accuracy based on that.  \n",
    "Because newgroup data consists of varieties of news or different types of text, features are not really correlated. And therefore Logistic Regression does not work as well as Naive Bayes does.  \n",
    "  \n",
    "c) In Logistic Regression, C value with minimum R-square is the best C value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) Train a logistic regression model. Find the 5 features with the largest weights for each label -- 20 features in total. Create a table with 20 rows and 4 columns that shows the weight for each of these features for each of the labels. Create the table again with bigram features. Any surprising features in this table?\n",
    "\n",
    "[5 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Category: \"alt.atheism\", top five features are: ['god tells' 'cheers kent' 'don think' 'alt atheism' 'promises broken']\n",
      "For Category: \"comp.graphics\", top five features are: ['does anybody' 'does know' '24 bit' 'comp graphics' 'thanks advance']\n",
      "For Category: \"sci.space\", top five features are: ['real life' 'space shuttle' 'nasa gov' 'sci space' 'space station']\n",
      "For Category: \"talk.religion.misc\", top five features are: ['sure wrong' 'ignorance strength' 'cheers kent' 'objective morality'\n",
      " 'jesus christ']\n",
      ": Feature Name        : alt.atheism    : comp.graphics :  sci.space  : talk.religion.misc\n",
      "==========================================================================================\n",
      ": god tells           : 0.666817       : -0.2833       : -0.286589   : -0.149177  \n",
      ": cheers kent         : 0.723716       : -0.742005     : -0.747503   : 0.809051   \n",
      ": don think           : 0.777132       : -0.459338     : -0.25303    : -0.056463  \n",
      ": alt atheism         : 0.809897       : -0.417891     : -0.418916   : 0.039521   \n",
      ": promises broken     : 0.901168       : -0.382413     : -0.385961   : -0.132651  \n",
      ": does anybody        : -0.228302      : 0.804144      : -0.345856   : -0.27264   \n",
      ": does know           : -0.473687      : 0.83835       : 0.073794    : -0.492409  \n",
      ": 24 bit              : -0.342922      : 0.937397      : -0.347235   : -0.282495  \n",
      ": comp graphics       : -0.435389      : 1.25863       : -0.520206   : -0.359166  \n",
      ": thanks advance      : -0.626034      : 1.622659      : -0.551841   : -0.517518  \n",
      ": real life           : -0.229573      : -0.276349     : 0.629798    : -0.131179  \n",
      ": space shuttle       : -0.303208      : -0.360386     : 0.867289    : -0.251299  \n",
      ": nasa gov            : -0.386741      : -0.225216     : 0.90143     : -0.364936  \n",
      ": sci space           : -0.383911      : -0.467439     : 1.114379    : -0.317572  \n",
      ": space station       : -0.414633      : -0.498794     : 1.197635    : -0.342401  \n",
      ": sure wrong          : -0.194762      : -0.235673     : -0.237939   : 0.69165    \n",
      ": ignorance strength  : -0.230412      : -0.272208     : -0.275216   : 0.799614   \n",
      ": cheers kent         : 0.723716       : -0.742005     : -0.747503   : 0.809051   \n",
      ": objective morality  : -0.039936      : -0.369754     : -0.373838   : 0.809901   \n",
      ": jesus christ        : -0.196877      : -0.337244     : -0.341149   : 0.895921   \n"
     ]
    }
   ],
   "source": [
    "## P4\n",
    "\n",
    "# define a function to find the top n features with largest weights for each label#\n",
    "def show_top_n(classifier, vectorizer, categories, n):\n",
    "    feature_names = np.asarray(vectorizer.get_feature_names())\n",
    "    feature_list = []\n",
    "    top = []\n",
    "    weight_aa = []\n",
    "    weight_cg = []\n",
    "    weight_ss = []\n",
    "    weight_trm = []\n",
    "    for i, category in enumerate(categories):\n",
    "        top_n = np.argsort(classifier.coef_[i])[-n:]\n",
    "        top.append(top_n)\n",
    "        print('For Category: ''\"%s\"'', top five features are: '\"%s\" % (category, feature_names[top_n]))\n",
    "        feature_list.append(feature_names[top_n])\n",
    "        for j, cat in enumerate(categories):\n",
    "            if cat == \"alt.atheism\":\n",
    "                weight_aa.append(classifier.coef_[j][top_n])               \n",
    "            elif cat == \"comp.graphics\":\n",
    "                weight_cg.append(classifier.coef_[j][top_n])\n",
    "            elif cat == \"sci.space\":\n",
    "                weight_ss.append(classifier.coef_[j][top_n])\n",
    "            elif cat == \"talk.religion.misc\":\n",
    "                weight_trm.append((classifier.coef_[j])[top_n])\n",
    "\n",
    "    return [feature_list, weight_aa, weight_cg, weight_ss, weight_trm]\n",
    "\n",
    "logit_tv = LogisticRegression().fit(train_tv, train_labels)\n",
    "\n",
    "#Call show_top_n function to show the top 5 features against each label#\n",
    "show_feature_list = show_top_n(logit_tv, tv, newsgroups_train.target_names, 5)\n",
    "\n",
    "\n",
    "# Create 20/4 table to display weight against each feature of the labels #\n",
    "\n",
    "print(\": Feature Name        : alt.atheism    : comp.graphics :  sci.space  : talk.religion.misc\")\n",
    "print(\"==========================================================================================\")\n",
    "\n",
    "for i in range(5):           \n",
    "    print(\":\",show_feature_list[0][0][i],\" \"*(18 - len(show_feature_list[0][0][i])),\n",
    "          \":\",round(show_feature_list[1][0][i],6),\" \"*(13 - len(str(round(show_feature_list[1][0][i],6)))),\n",
    "          \":\",round(show_feature_list[2][0][i],6),\" \"*(12 - len(str(round(show_feature_list[2][0][i],6)))),\n",
    "          \":\",round(show_feature_list[3][0][i],6),\" \"*(10 - len(str(round(show_feature_list[3][0][i],6)))),\n",
    "          \":\",round(show_feature_list[4][0][i],6),\" \"*(10 - len(str(round(show_feature_list[4][0][i],6)))))\n",
    "\n",
    "for i in range(5):           \n",
    "    print(\":\",show_feature_list[0][1][i],\" \"*(18 - len(show_feature_list[0][1][i])),\n",
    "          \":\",round(show_feature_list[1][1][i],6),\" \"*(13 - len(str(round(show_feature_list[1][1][i],6)))),\n",
    "          \":\",round(show_feature_list[2][1][i],6),\" \"*(12 - len(str(round(show_feature_list[2][1][i],6)))),\n",
    "          \":\",round(show_feature_list[3][1][i],6),\" \"*(10 - len(str(round(show_feature_list[3][1][i],6)))),\n",
    "          \":\",round(show_feature_list[4][1][i],6),\" \"*(10 - len(str(round(show_feature_list[4][1][i],6)))))\n",
    "    \n",
    "for i in range(5):           \n",
    "    print(\":\",show_feature_list[0][2][i],\" \"*(18 - len(show_feature_list[0][2][i])),\n",
    "          \":\",round(show_feature_list[1][2][i],6),\" \"*(13 - len(str(round(show_feature_list[1][2][i],6)))),\n",
    "          \":\",round(show_feature_list[2][2][i],6),\" \"*(12 - len(str(round(show_feature_list[2][2][i],6)))),\n",
    "          \":\",round(show_feature_list[3][2][i],6),\" \"*(10 - len(str(round(show_feature_list[3][2][i],6)))),\n",
    "          \":\",round(show_feature_list[4][2][i],6),\" \"*(10 - len(str(round(show_feature_list[4][2][i],6)))))\n",
    "    \n",
    "for i in range(5):           \n",
    "    print(\":\",show_feature_list[0][3][i],\" \"*(18 - len(show_feature_list[0][3][i])),\n",
    "          \":\",round(show_feature_list[1][3][i],6),\" \"*(13 - len(str(round(show_feature_list[1][3][i],6)))),\n",
    "          \":\",round(show_feature_list[2][3][i],6),\" \"*(12 - len(str(round(show_feature_list[2][3][i],6)))),\n",
    "          \":\",round(show_feature_list[3][3][i],6),\" \"*(10 - len(str(round(show_feature_list[3][3][i],6)))),\n",
    "          \":\",round(show_feature_list[4][3][i],6),\" \"*(10 - len(str(round(show_feature_list[4][3][i],6)))))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER: Other than five top features in each label, co-efficients of other features are in negative values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5) Try to improve the logistic regression classifier by passing a custom preprocessor to CountVectorizer. The preprocessing function runs on the raw text, before it is split into words by the tokenizer. Your preprocessor should try to normalize the input in various ways to improve generalization. For example, try lowercasing everything, replacing sequences of numbers with a single token, removing various other non-letter characters, and shortening long words. If you're not already familiar with regular expressions for manipulating strings, see https://docs.python.org/2/library/re.html, and re.sub() in particular. With your new preprocessor, how much did you reduce the size of the dictionary?\n",
    "\n",
    "For reference, I was able to improve dev F1 by 2 points.\n",
    "\n",
    "[4 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of dictionary on raw data:  26879\n",
      "F1 score on Dev_data without any prior manipulation 0.6793260443571856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/debalinamaiti/Documents/GraduateStudy/MIDS/Python_Bridge/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:286: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['afterw', 'agains', 'alread', 'althou', 'amongs', 'amoung', 'anothe', 'anythi', 'anywhe', 'becaus', 'becomi', 'betwee', 'couldn', 'descri', 'elsewh', 'everyo', 'everyt', 'everyw', 'fiftee', 'furthe', 'hereaf', 'hereup', 'hersel', 'himsel', 'howeve', 'hundre', 'intere', 'meanwh', 'moreov', 'neithe', 'nevert', 'nothin', 'nowher', 'otherw', 'oursel', 'perhap', 'seemin', 'seriou', 'severa', 'sincer', 'someho', 'someon', 'someth', 'someti', 'somewh', 'themse', 'therea', 'thereb', 'theref', 'therei', 'thereu', 'throug', 'togeth', 'whatev', 'whenev', 'wherea', 'whereb', 'wherei', 'whereu', 'wherev', 'whethe', 'whithe', 'whoeve', 'withou', 'yourse'] not in stop_words.\n",
      "  sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of dictionary after pre-processing:  17512\n",
      "f1 score with better preprocessor:  0.7193989758105468\n"
     ]
    }
   ],
   "source": [
    "### P5():\n",
    "\n",
    "def my_preprocessor(data):\n",
    "\n",
    "   #Lowercase characters\n",
    "   data= data.lower()\n",
    "   #Remove non-letter or non number characters\n",
    "   data = re.sub(\"A-Za-z0-9 \\\\n\",\" \",data)\n",
    "   #shorten long words to 6 characters max\n",
    "   data=re.sub(\"(\\w{%d})\\w+\" %6,\"\\\\1\",data)\n",
    "   #replace sequence with number\n",
    "   data = re.sub(\"([\\d]+)\",\" number \",data)\n",
    "   return data\n",
    "\n",
    "\n",
    "# Print F1 score before processing the raw data\n",
    "f1_original = logit.predict(dev_data_cv)\n",
    "print(\"The size of dictionary on raw data: \", train_cv.shape[1] )\n",
    "print(\"F1 score on Dev_data without any prior manipulation\",f1_score(dev_labels, f1_original, average = 'macro'))\n",
    "\n",
    "\n",
    "# Process raw data by calling pre-processor\n",
    "processed_cv = CountVectorizer(preprocessor=my_preprocessor, stop_words='english')\n",
    "processed_train_cv = processed_cv.fit_transform(train_data)\n",
    "processed_dev_cv =processed_cv.transform(dev_data)\n",
    "processed_logit_cv = LogisticRegression(penalty='l2')\n",
    "processed_logit_cv.fit(processed_train_cv,train_labels)\n",
    "f1_later = processed_logit_cv.predict(processed_dev_cv)\n",
    "print(\"The size of dictionary after pre-processing: \", processed_train_cv.shape[1] )\n",
    "print(\"f1 score with better preprocessor: \", metrics.f1_score(dev_labels,f1_later,average ='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANS:** I was able to reduce teh size of dictionary by 9357 points.  \n",
    "F1 score increased by 0.04 points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(6) The idea of regularization is to avoid learning very large weights (which are likely to fit the training data, but not generalize well) by adding a penalty to the total size of the learned weights. That is, logistic regression seeks the set of weights that minimizes errors in the training data AND has a small size. The default regularization, L2, computes this size as the sum of the squared weights (see P3, above). L1 regularization computes this size as the sum of the absolute values of the weights. The result is that whereas L2 regularization makes all the weights relatively small, L1 regularization drives lots of the weights to 0, effectively removing unimportant features.\n",
    "\n",
    "Train a logistic regression model using a \"l1\" penalty. Output the number of learned weights that are not equal to zero. How does this compare to the number of non-zero weights you get with \"l2\"? Now, reduce the size of the vocabulary by keeping only those features that have at least one non-zero weight and retrain a model using \"l2\".\n",
    "\n",
    "Make a plot showing accuracy of the re-trained model vs. the vocabulary size you get when pruning unused features by adjusting the C parameter.\n",
    "\n",
    "Note: The gradient descent code that trains the logistic regression model sometimes has trouble converging with extreme settings of the C parameter. Relax the convergence criteria by setting tol=.01 (the default is .0001).\n",
    "\n",
    "[4 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/debalinamaiti/Documents/GraduateStudy/MIDS/Python_Bridge/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of non-zero weights for L1 regularization:  1814\n",
      "Number of non-zero weights for L2 regularization:  107516\n",
      "As number of samples where weight is  0 , not manipulating vocabulary size\n",
      "Vocabulary size for dev data is:  16246\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# def P6():\n",
    "# Keep this random seed here to make comparison easier.\n",
    "np.random.seed(0)\n",
    "\n",
    "#Classifier using L1 penalty\n",
    "logit_l1 = LogisticRegression(penalty='l1', tol=0.01, C=1).fit(train_cv, train_labels)\n",
    "print('Number of non-zero weights for L1 regularization: ',np.count_nonzero(logit_l1.coef_ != 0))\n",
    "\n",
    "#Classifier using L2 penalty\n",
    "logit_l2 = LogisticRegression(penalty='l2', tol=0.01, C=1).fit(train_cv, train_labels)\n",
    "print('Number of non-zero weights for L2 regularization: ',np.count_nonzero(logit_l2.coef_ != 0))\n",
    "\n",
    "#Reduce vocab size keeping only those features that have at least one non-zero weight and retrain a model using \"l2\"\n",
    "\n",
    "print('As number of samples where weight is ',np.count_nonzero(logit_l2.coef_ == 0),', not manipulating vocabulary size',)\n",
    "\n",
    "#As there is no zero weight, we will be using logit_l2 classifier\n",
    "\n",
    "# Plot Accuracy vs vocab size on dev data with variable c values\n",
    "c_values = [1 , 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "accuracy_score = [1 , 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "vocab_size = [1 , 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "for c in c_values:\n",
    "    logit_plot = LogisticRegression (C = c, penalty='l2', tol=0.01,).fit(train_cv, train_labels)\n",
    "    accuracy_score[c-1] = logit_plot.score(dev_data_cv, dev_labels)\n",
    "\n",
    "#dev_feature_names = cv.get_feature_names()\n",
    "#vocab_size = len(feature_names)\n",
    "print('Vocabulary size for dev data is: ', vocab_dev_len)\n",
    "plt.xlabel('C values')\n",
    "plt.ylabel('Accuracies')\n",
    "plt.scatter(c_values, accuracy_score,  color='black')\n",
    "plt.plot(c_values, accuracy_score, color='blue', linewidth=3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(7) Use the TfidfVectorizer -- how is this different from the CountVectorizer? Train a logistic regression model with C=100.\n",
    "\n",
    "Make predictions on the dev data and show the top 3 documents where the ratio R is largest, where R is:\n",
    "\n",
    "maximum predicted probability / predicted probability of the correct label\n",
    "\n",
    "What kinds of mistakes is the model making? Suggest a way to address one particular issue that you see.\n",
    "\n",
    "[4 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this case predicted probability of the correct label is fixed; so R will be best for the labels with 3 highest predicted probabilities.\n",
      "Three documents where ratio R is largest are:  talk.religion.misc   comp.graphics   sci.space\n"
     ]
    }
   ],
   "source": [
    "###def P7():\n",
    "logit_tvC100 = LogisticRegression(C=100).fit(train_tv, train_labels)\n",
    "dev_data_tv = tv.transform(dev_data)\n",
    "pred_dev = logit_tvC100.predict(dev_data_tv)\n",
    "pred_prob = logit_tvC100.predict_proba(dev_data_tv)\n",
    "\n",
    "# Calculate max predicted probability and return label with lowest predicted probability\n",
    "def best3_pred_prob_label():\n",
    "    lowest_pred_prob = 1\n",
    "    worst_predicted_label = ''\n",
    "    best_predicted_labels = [1, 2, 3]\n",
    "    for n, cat in enumerate(categories):\n",
    "        if cat == \"alt.atheism\":\n",
    "            max_pred_prob_aa = max(pred_prob[n])\n",
    "            if max_pred_prob_aa < lowest_pred_prob:\n",
    "                lowest_pred_prob = max_pred_prob_aa\n",
    "                worst_predicted_label = cat\n",
    "        elif cat == \"comp.graphics\":\n",
    "            max_pred_prob_cg = max(pred_prob[n])\n",
    "            if max_pred_prob_cg < lowest_pred_prob:\n",
    "                lowest_pred_prob = max_pred_prob_cg\n",
    "                worst_predicted_label = cat\n",
    "        elif cat == \"sci.space\":\n",
    "            max_pred_prob_ss = max(pred_prob[n])\n",
    "            if max_pred_prob_ss < lowest_pred_prob:\n",
    "                lowest_pred_prob = max_pred_prob_ss\n",
    "                worst_predicted_label = cat\n",
    "        elif cat == \"talk.religion.misc\":\n",
    "            max_pred_prob_trm = max(pred_prob[n])\n",
    "            if max_pred_prob_trm < lowest_pred_prob:\n",
    "                lowest_pred_prob = max_pred_prob_trm\n",
    "                worst_predicted_label = cat\n",
    "    j = 0\n",
    "    for category in categories:\n",
    "        if category != worst_predicted_label:\n",
    "            best_predicted_labels[j] = category\n",
    "            j = j + 1\n",
    "    return best_predicted_labels\n",
    "# Calculate predicted probability of the correct label\n",
    "\n",
    "best3_pp_labels = best3_pred_prob_label()\n",
    "\n",
    "print(\"In this case predicted probability of the correct label is fixed;\"\n",
    "      \" so R will be best for the labels with 3 highest predicted probabilities.\")\n",
    "\n",
    "print(\"Three documents where ratio R is largest are: \", best3_pp_labels[0], ' ', best3_pp_labels[1], ' ', best3_pp_labels[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER: Model is predicting the label incorrectly. For example for graphics technology is being predicted as a document under category \"alt.atheism\", which should correctly be a \"comp.graphics\" doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(8) EXTRA CREDIT\n",
    "\n",
    "Try implementing one of your ideas based on your error analysis. Use logistic regression as your underlying model.\n",
    "\n",
    "- [1 pt] for a reasonable attempt\n",
    "- [2 pts] for improved performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
